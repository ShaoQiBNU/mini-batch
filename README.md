机器学习中参数更新的方法
========================

> 机器学习中参数更新的方法有三种：

# 1. Batch Gradient Descent 批梯度下降

> 在梯度下降中需要对所有样本进行处理，遍历全部数据集计算一次损失函数，进行一次参数更新，这样得到的方向能够更加准确的指向极值的方向，但是计算开销大，速度慢；假如有500万，甚至5000万个样本的话走一轮迭代就会非常的耗时。这个时候的梯度下降叫做full batch。 

# 2. Stochastic Gradient Descent 随机梯度下降

> 对每一个样本计算一次损失函数，进行一次参数更新，优点是速度快，缺点是方向波动大，忽东忽西，不能准确的指向极值的方向，有时甚至两次更新相互抵消；

# 3. Mini-batch Gradient Decent 小批梯度下降


> 前面两种方法的折中，把样本数据分为若干批，分批来计算损失函数和更新参数，这样方向比较稳定，计算开销也相对较小。Batch Size就是每一批的样本数量，可以把样本分成等量的子集。 例如把100万样本分成1000份， 每份1000个样本， 这些子集就称为mini batch。然后分别用一个for循环遍历这1000个子集。 针对每一个子集做一次梯度下降，然后更新参数w和b的值，就是一次Iteration。接着到下一个子集中继续进行梯度下降。 这样在遍历完所有的mini batch之后相当于在梯度下降中做了1000次迭代。 将遍历一次所有样本的行为叫做一个 epoch，也就是一个世代。 在mini batch下的梯度下降中做的事情其实跟full batch一样，只不过训练的数据不再是所有的样本，而是一个个的子集。 这样在mini batch在一个epoch中就能进行1000次的梯度下降，而在full batch中只有一次。 这样就大大的提高了算法的运行速度。

> 既然有了mini batch，那就会有一个batch size的超参数，也就是块大小。代表着每一个mini batch中有多少个样本。 我们一般设置为2的n次方。 例如64,128,512,1024. 一般不会超过这个范围。不能太大，因为太大了会无限接近full batch的行为，速度会慢。 也不能太小，太小了以后可能算法永远不会收敛。 当然如果我们的数据比较小， 但也用不着mini batch了。 full batch的效果是最好的。




